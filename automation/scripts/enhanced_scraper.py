#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Scraper Module - Scraping n√¢ng cao v·ªõi JavaScript optimization
Handles: enhanced data extraction, JavaScript acceleration, progress tracking
"""

import time
from datetime import datetime
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException


class EnhancedScraper:
    """Class x·ª≠ l√Ω scraping n√¢ng cao v·ªõi t·ªëi ∆∞u JavaScript"""

    def __init__(self, driver, logger):
        self.driver = driver
        self.logger = logger

    def scrape_order_data_basic(self):
        """L·∫•y d·ªØ li·ªáu ƒë∆°n h√†ng c∆° b·∫£n v·ªõi JavaScript acceleration"""
        try:
            self.logger.info("üìä B·∫Øt ƒë·∫ßu l·∫•y d·ªØ li·ªáu ƒë∆°n h√†ng c∆° b·∫£n...")

            start_time = time.time()
            orders = []

            # S·ª≠ d·ª•ng JavaScript ƒë·ªÉ tƒÉng t·ªëc truy v·∫•n DOM
            js_script = """
            return Array.from(document.querySelectorAll('table tbody tr, tbody tr, .table tbody tr'))
                .filter(row => row.querySelectorAll('td').length > 0)
                .map(row => {
                    const cells = Array.from(row.querySelectorAll('td'));
                    return cells.map(cell => cell.innerText.trim());
                });
            """

            try:
                # Ch·ªù ng·∫Øn cho DOM ·ªïn ƒë·ªãnh
                time.sleep(0.3)

                # Th·ª±c thi script JS ƒë·ªÉ l·∫•y d·ªØ li·ªáu tr·ª±c ti·∫øp - nhanh h∆°n nhi·ªÅu
                rows_data = self.driver.execute_script(js_script)

                if not rows_data or len(rows_data) == 0:
                    self.logger.error("‚ùå Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu th√¥ng qua JavaScript")

                    # Fallback sang c√°ch c≈© n·∫øu JS kh√¥ng ho·∫°t ƒë·ªông
                    rows_data = self._fallback_scraping_method()

                if rows_data:
                    orders = self._process_rows_data(rows_data)

            except Exception as e:
                self.logger.error(f"‚ùå L·ªói JavaScript scraping: {e}")
                # Fallback method
                orders = self._fallback_scraping_method()

            duration = time.time() - start_time
            self.logger.info(f"‚úÖ L·∫•y ƒë∆∞·ª£c {len(orders)} ƒë∆°n h√†ng trong {duration:.2f}s")
            return orders

        except Exception as e:
            self.logger.error(f"‚ùå L·ªói scrape data c∆° b·∫£n: {e}")
            return []

    def _fallback_scraping_method(self):
        """Ph∆∞∆°ng ph√°p fallback khi JavaScript fail"""
        try:
            self.logger.info("üîÑ S·ª≠ d·ª•ng fallback scraping method...")

            simple_selectors = ["table tbody tr", "tbody tr", ".table tbody tr"]
            rows_data = []

            for selector in simple_selectors:
                try:
                    rows = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if rows:
                        for row in rows:
                            cells = row.find_elements(By.CSS_SELECTOR, "td")
                            if len(cells) > 0:
                                cell_texts = [cell.text.strip() for cell in cells]
                                rows_data.append(cell_texts)
                        break
                except Exception:
                    continue

            return self._process_rows_data(rows_data)

        except Exception as e:
            self.logger.error(f"‚ùå Fallback method failed: {e}")
            return []

    def _process_rows_data(self, rows_data):
        """X·ª≠ l√Ω raw rows data th√†nh structured orders"""
        try:
            orders = []

            for i, row_cells in enumerate(rows_data):
                if len(row_cells) < 3:  # Skip invalid rows
                    continue

                # T·∫°o order data structure
                order_data = {
                    'row_index': i + 1,
                    'total_columns': len(row_cells),
                    'scraped_at': datetime.now().isoformat()
                }

                # Map cells to columns
                for j, cell_text in enumerate(row_cells):
                    order_data[f'col_{j+1}'] = cell_text

                # Extract order ID using various patterns
                order_id = self._extract_order_id(row_cells)
                if order_id:
                    order_data['id'] = order_id

                # Extract customer name (usually in specific columns)
                customer = self._extract_customer_name(row_cells)
                if customer:
                    order_data['customer'] = customer

                # Extract order code (usually first clickable column)
                order_code = self._extract_order_code(row_cells)
                if order_code:
                    order_data['order_code'] = order_code

                orders.append(order_data)

                # Progress logging
                if (i + 1) % 50 == 0:
                    self.logger.info(f"‚ö° Processed {i + 1}/{len(rows_data)} rows")

            return orders

        except Exception as e:
            self.logger.error(f"‚ùå Error processing rows data: {e}")
            return []

    def _extract_order_id(self, row_cells):
        """Extract order ID from row cells"""
        try:
            # Try to find order ID patterns
            for cell in row_cells[:3]:  # Usually in first 3 columns
                if cell and cell.isdigit() and len(cell) >= 4:
                    return cell
            return None
        except:
            return None

    def _extract_customer_name(self, row_cells):
        """Extract customer name from row cells"""
        try:
            # Customer name usually in columns 4-6
            for i in range(3, min(7, len(row_cells))):
                if i < len(row_cells) and row_cells[i]:
                    cell = row_cells[i].strip()
                    if cell and len(cell) > 2 and not cell.isdigit():
                        return cell
            return None
        except:
            return None

    def _extract_order_code(self, row_cells):
        """Extract order code from row cells"""
        try:
            # Order code usually in first column
            if len(row_cells) > 0 and row_cells[0]:
                return row_cells[0].strip()
            return None
        except:
            return None

    def enhanced_scrape_order_data(self):
        """Enhanced scraping v·ªõi order ID extraction n√¢ng cao"""
        try:
            self.logger.info("üìä B·∫Øt ƒë·∫ßu l·∫•y d·ªØ li·ªáu ƒë∆°n h√†ng ENHANCED...")

            # Step 1: Get basic order data
            orders = self.scrape_order_data_basic()

            if not orders:
                self.logger.warning("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu c∆° b·∫£n ƒë·ªÉ enhance")
                return []

            # Step 2: Extract order IDs cho vi·ªác l·∫•y chi ti·∫øt sau n√†y
            order_ids = []
            orders_with_ids = []

            for order in orders:
                order_id = order.get('id')
                if order_id:
                    order_ids.append(order_id)
                    orders_with_ids.append(order)
                else:
                    # Try harder to extract ID from DOM
                    enhanced_id = self._extract_id_from_dom(order)
                    if enhanced_id:
                        order['id'] = enhanced_id
                        order_ids.append(enhanced_id)
                        orders_with_ids.append(order)

            self.logger.info(f"üì¶ T√¨m th·∫•y {len(order_ids)} order IDs t·ª´ {len(orders)} ƒë∆°n h√†ng")

            # Step 3: Add metadata for further processing
            for order in orders_with_ids:
                order['has_id'] = True
                order['ready_for_enhancement'] = True

            self.logger.info(f"‚úÖ Enhanced {len(orders_with_ids)} ƒë∆°n h√†ng v·ªõi order IDs")
            return {
                'orders': orders_with_ids,
                'order_ids': order_ids,
                'total_orders': len(orders),
                'enhanced_orders': len(orders_with_ids),
                'enhancement_rate': len(orders_with_ids) / len(orders) * 100 if orders else 0
            }

        except Exception as e:
            self.logger.error(f"‚ùå L·ªói enhanced scraping: {e}")
            return {
                'orders': [],
                'order_ids': [],
                'total_orders': 0,
                'enhanced_orders': 0,
                'enhancement_rate': 0
            }

    def _extract_id_from_dom(self, order):
        """Tr√≠ch xu·∫•t order ID t·ª´ DOM b·∫±ng JavaScript n√¢ng cao"""
        try:
            # S·ª≠ d·ª•ng row index ƒë·ªÉ t√¨m order ID t·ª´ DOM
            row_index = order.get('row_index', 0)
            if row_index == 0:
                return None

            js_script = f"""
            try {{
                const rows = document.querySelectorAll('table tbody tr, tbody tr, .table tbody tr');
                if (rows.length >= {row_index}) {{
                    const targetRow = rows[{row_index - 1}];
                    const links = targetRow.querySelectorAll('a[href*="/so/detail/"]');
                    if (links.length > 0) {{
                        const href = links[0].getAttribute('href');
                        const match = href.match(/\\/so\\/detail\\/(\\d+)/);
                        return match ? match[1] : null;
                    }}
                }}
                return null;
            }} catch(e) {{
                return null;
            }}
            """

            result = self.driver.execute_script(js_script)
            return result

        except Exception as e:
            self.logger.debug(f"‚ö†Ô∏è Kh√¥ng th·ªÉ extract ID t·ª´ DOM cho row {order.get('row_index', 'unknown')}: {e}")
            return None

    def get_table_structure_info(self):
        """L·∫•y th√¥ng tin c·∫•u tr√∫c b·∫£ng ƒë·ªÉ debug"""
        try:
            js_script = """
            const tables = document.querySelectorAll('table');
            const result = [];

            tables.forEach((table, index) => {
                const headers = Array.from(table.querySelectorAll('thead th, tbody tr:first-child td'))
                    .map(th => th.textContent.trim());
                const rowCount = table.querySelectorAll('tbody tr').length;

                result.push({
                    tableIndex: index,
                    headers: headers,
                    rowCount: rowCount,
                    id: table.id || 'no-id',
                    className: table.className || 'no-class'
                });
            });

            return result;
            """

            structure_info = self.driver.execute_script(js_script)
            self.logger.info(f"üìã Table structure info: {structure_info}")
            return structure_info

        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Cannot get table structure: {e}")
            return []

    def wait_for_table_load(self, timeout=30):
        """Ch·ªù b·∫£ng d·ªØ li·ªáu load ho√†n to√†n"""
        try:
            self.logger.info("‚è≥ Ch·ªù b·∫£ng d·ªØ li·ªáu load...")

            # Wait for table presence
            WebDriverWait(self.driver, timeout).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "table tbody tr"))
            )

            # Wait for AJAX completion if jQuery is available
            try:
                WebDriverWait(self.driver, timeout).until(
                    lambda driver: driver.execute_script("return typeof $ !== 'undefined' && $.active == 0")
                )
                self.logger.info("‚úÖ AJAX requests completed")
            except:
                self.logger.info("‚ö†Ô∏è jQuery not available, skipping AJAX wait")

            # Additional stability wait
            time.sleep(1)

            self.logger.info("‚úÖ B·∫£ng d·ªØ li·ªáu ƒë√£ load ho√†n to√†n")
            return True

        except TimeoutException:
            self.logger.error(f"‚ùå Timeout waiting for table load ({timeout}s)")
            return False
        except Exception as e:
            self.logger.error(f"‚ùå Error waiting for table load: {e}")
            return False


    def extract_single_page_data(self):
        """
        üìÑ Extract data t·ª´ trang hi·ªán t·∫°i (s·ª≠ d·ª•ng cho pagination)

        Returns:
            list: Orders data from current page
        """
        try:
            # Wait for table to be ready
            if not self.wait_for_table_load(timeout=15):
                self.logger.warning("‚ö†Ô∏è Table load timeout for current page")
                return []

            # Extract data from current page only
            orders = self.scrape_order_data_basic()
            return orders

        except Exception as e:
            self.logger.error(f"‚ùå Error extracting single page data: {e}")
            return []

    def enhanced_scrape_all_pages(self):
        """
        üìä Enhanced scraping v·ªõi pagination - l·∫•y h·∫øt t·∫•t c·∫£ trang

        Returns:
            dict: Complete extraction result with all pages data
        """
        try:
            from scripts.pagination_handler import PaginationHandler

            self.logger.info("üìä Starting enhanced scraping with pagination...")

            # Initialize pagination handler
            pagination_handler = PaginationHandler(self.driver, self.logger)

            # Get total records estimate
            page_estimate = pagination_handler.quick_page_count_estimate()
            if page_estimate['estimated_pages'] > 0:
                self.logger.info(f"üìä Estimated: {page_estimate['estimated_pages']} pages for {page_estimate['total_records']:,} records")

            # Extract data from all pages
            result = pagination_handler.extract_all_pages_data(
                extract_function=self.extract_single_page_data,
                max_pages=50  # Safety limit
            )

            if not result['success']:
                self.logger.error("‚ùå Pagination extraction failed")
                return {
                    'orders': [],
                    'order_ids': [],
                    'total_orders': 0,
                    'enhanced_orders': 0,
                    'enhancement_rate': 0,
                    'pages_processed': 0,
                    'success': False
                }

            # Process all collected orders
            all_orders = result['all_data']
            order_ids = []
            enhanced_orders = []

            # Extract order IDs and enhance data
            for order in all_orders:
                order_id = order.get('id')
                if order_id:
                    order_ids.append(order_id)
                    enhanced_orders.append(order)
                    order['has_id'] = True
                    order['ready_for_enhancement'] = True

            enhancement_rate = len(enhanced_orders) / len(all_orders) * 100 if all_orders else 0

            self.logger.info(f"üìä Complete extraction summary:")
            self.logger.info(f"   üì¶ Total orders: {len(all_orders):,}")
            self.logger.info(f"   üÜî Enhanced orders: {len(enhanced_orders):,}")
            self.logger.info(f"   üìà Enhancement rate: {enhancement_rate:.1f}%")
            self.logger.info(f"   üìÑ Pages processed: {result['pages_processed']}")
            self.logger.info(f"   üìä Completion rate: {result['completion_rate']:.1f}%")

            return {
                'orders': enhanced_orders,
                'order_ids': order_ids,
                'total_orders': len(all_orders),
                'enhanced_orders': len(enhanced_orders),
                'enhancement_rate': enhancement_rate,
                'pages_processed': result['pages_processed'],
                'completion_rate': result['completion_rate'],
                'total_expected': result['total_expected'],
                'success': True
            }

        except Exception as e:
            self.logger.error(f"‚ùå Enhanced scraping with pagination failed: {e}")
            return {
                'orders': [],
                'order_ids': [],
                'total_orders': 0,
                'enhanced_orders': 0,
                'enhancement_rate': 0,
                'pages_processed': 0,
                'success': False,
                'error': str(e)
            }


def enhanced_scrape_orders(driver, logger):
    """Convenience function ƒë·ªÉ scrape enhanced orders (single page)"""
    scraper = EnhancedScraper(driver, logger)

    # Wait for table to load first
    if not scraper.wait_for_table_load():
        return {
            'success': False,
            'error': 'Table load timeout',
            'orders': [],
            'order_ids': []
        }

    # Get table structure for debugging
    scraper.get_table_structure_info()

    # Perform enhanced scraping
    result = scraper.enhanced_scrape_order_data()

    return {
        'success': len(result['orders']) > 0,
        'orders': result['orders'],
        'order_ids': result['order_ids'],
        'total_orders': result['total_orders'],
        'enhanced_orders': result['enhanced_orders'],
        'enhancement_rate': result['enhancement_rate']
    }


def enhanced_scrape_all_orders(driver, logger):
    """Convenience function ƒë·ªÉ scrape ALL orders v·ªõi pagination"""
    scraper = EnhancedScraper(driver, logger)

    # Wait for table to load first
    if not scraper.wait_for_table_load():
        return {
            'success': False,
            'error': 'Table load timeout',
            'orders': [],
            'order_ids': []
        }

    # Get table structure for debugging
    scraper.get_table_structure_info()

    # Perform complete enhanced scraping with pagination
    result = scraper.enhanced_scrape_all_pages()

    return result
